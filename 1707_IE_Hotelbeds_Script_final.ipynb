{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.tuigroup.com/damfiles/default/tuigroup-15/en/media/images-press-releases/logo_hotelbeds_group-98bdb34cb55c8a3138b8e0aa614e2c30.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"https://www.tuigroup.com/damfiles/default/tuigroup-15/en/media/images-press-releases/logo_hotelbeds_group-98bdb34cb55c8a3138b8e0aa614e2c30.jpg\" ,width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.warn { background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>div.warn { background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon - Working File\n",
    "<div class=\"warn\">\n",
    "*Institution:* IE HST - Master Business Analytics <br />\n",
    "*Authour:* Group F, Lead: Shaurya Rawat <br />\n",
    "*Date:* May/2017 <br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents | Summary\n",
    "<a href='#gm'>1.General Remarks'</a> <br>\n",
    "<a href='#lib'>2.Libraries and UDF'</a> <br>\n",
    "<a href='#3'>3. Load and Pre-Process Table: Clients '</a> <br>\n",
    "<a href='#4'>4. Load, Pre-Process and Join tables: Clients and 1st Booking'</a> <br>\n",
    "<a href='#5'>5. Load and Pre-Process Data: Bookings Transfer'</a> <br>\n",
    "<a href='#6'>6. Load, Pre-Process and Incidents | Join  Bookings'</a> <br>\n",
    "<a href='#7'>7. Aggregate Bookings_Transfer and Join: Clients_1'</a> <br>\n",
    "<a href='#8'>8. Load and Pre-Process Data: Bookings Activities'</a> <br>\n",
    "<a href='#9'>9. Join Incidents, Promotional Deals and Bundles to booking_act'</a> <br>\n",
    "<a href='#10'>10. Load, Pre-Process and Join Booking_Act: Products'</a> <br>\n",
    "<a href='#11'>11. Store booking_act_2 to HIVE '</a> <br>\n",
    "<a href='#12'>12. Aggregate Bookings_Activities and Join: Clients_2'</a> <br>\n",
    "<a href='#13'>13. Load, Pre-Process and Join Clients_3: Demand Transfer'</a> <br>\n",
    "<a href='#14'>14. Load, Pre-Process and Join Clients_4: Demand Activities'</a> <br>\n",
    "<a href='#15'>15. Consolidate Data for Model'</a> <br>\n",
    "<a href='#16'>16. High-level validation'</a> <br>\n",
    "<a href='#17'>17. Code - Backlog'</a> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General Remarks <a id='gr'></a>\n",
    "<div class=\"warn\">\n",
    "Data is loaded from cluster and processed as dataframe with Pyspark. Although I am used and faster in programming in pandas, I tried to avoid conversion to PANDAS-DF, to allocate ressources better. In the following chuncks data is loaded, cleaned and combined. Different outputs are intermediate, for EDA or finally used for the modelling of customer segmentation. Libraries and UDF are loaded / defined right in the beginning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Libraries and UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################  IMPORT LIBRARIES ######################################\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "#for date transformation\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf, substring, desc, concat, lit, date_format, dayofmonth, regexp_replace, trim, col, lower\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#for data type transformation\n",
    "from pyspark.sql.types import DateType,DoubleType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "#####################################  USER DEFINED FUNCTIONS ######################################\n",
    "#cast to date\n",
    "func =  udf (lambda x: datetime.strptime(x, '%d%m%Y'), DateType())\n",
    "\n",
    "\n",
    "# count missing data\n",
    "def count_nulls(df):\n",
    "    null_counts = []          #make an empty list to hold our results\n",
    "    for col in df.dtypes:     #iterate through the column data types we saw above, e.g. ('C0', 'bigint')\n",
    "        cname = col[0]        #splits out the column name, e.g. 'C0'    \n",
    "        ctype = col[1]        #splits out the column type, e.g. 'bigint'\n",
    "        if ctype != 'string': #skip processing string columns for efficiency (can't have nulls)\n",
    "            nulls = df.where( df[cname].isNull() ).count()\n",
    "            result = tuple([cname, nulls])  #new tuple, (column name, null count)\n",
    "            null_counts.append(result)      #put the new tuple in our result list\n",
    "    return null_counts\n",
    "\n",
    "\n",
    "#to add up columns\n",
    "def increase (old, new): return old+new\n",
    "udfIncrease=udf(increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "all included in the beginning, please add libraries only here\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Pre-Process Table: Clients <a id='3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data # clean headers\n",
    "clients = spark.read.csv(\"/datathon/IE_Challenge_3.Clients v2.txt\", header=True, sep=\"#\")\n",
    "for name in clients.schema.names: clients = clients.withColumnRenamed(name, name.replace(' ', '_'))\n",
    "for name in clients.schema.names: clients = clients.withColumnRenamed(name, name.replace('(', ''))\n",
    "for name in clients.schema.names: clients = clients.withColumnRenamed(name, name.replace(')', ''))\n",
    "for name in clients.schema.names: clients = clients.withColumnRenamed(name, name.lower())\n",
    "clients = clients.withColumnRenamed('database_target_formula','lifecycle')\n",
    "clients = clients.withColumnRenamed('country_commercial','country_home')\n",
    "clients = clients.withColumnRenamed('market_country','country_market')\n",
    "\n",
    "\n",
    "# create key (for booking, 1st booking, demand) and general unique identifier and add simple counter\n",
    "clients_0 = clients.withColumn(\"pk_client\",F.concat(col(\"atlas_number\"), lit(\"\"), col(\"branch_number\")))\n",
    "clients_0 = clients_0.withColumn('count', lit(1))\n",
    "                        \n",
    "    \n",
    "# create int features for segmentation\n",
    "clients_0 = clients_0.withColumn(\"lifecycle\",F.substring(clients_0.lifecycle, 0, 1))\n",
    "\n",
    "\n",
    "# remove not-needed features and duplicates (we need one too many relationship for date modelling part)\n",
    "# atlas_code_distribution_type_details is poorly maintained \n",
    "# clients.select('atlas_code_distribution_type_details').show(5000)\n",
    "drop_list = ['atlas_code_distribution_type_details','atlas_number','branch_number','commercial_stage','atlas_status','homeworker','inactive_reason','inactive_date','acquisition_channel_type','acquisition_channel_description','customer_acquisition_date','city_commercial','post_code_commercial']\n",
    "clients_0 = clients_0.select([column for column in clients_0.columns if column not in drop_list])\n",
    "clients_0 = clients_0.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "With the above chunk we loaded and cleaned all data from the client table.\n",
    "This dataframe contains all keys needed to connect other information.  \n",
    "Output is the df __clients_0__.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load, Pre-Process and Join tables: Clients and 1st Booking <a id='4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load data # clean column names\n",
    "first_bookings = spark.read.csv(\"/datathon/IE_Challenge_5.ClientDate1stBooking v2.txt\", header=True, sep=\"#\")\n",
    "for name in first_bookings.schema.names: first_bookings = first_bookings.withColumnRenamed(name, name.lower())\n",
    "for name in first_bookings.schema.names: first_bookings = first_bookings.withColumnRenamed(name, name.replace(' ', '_'))\n",
    "first_bookings = first_bookings.withColumnRenamed('product', 'first_product') #ambigous\n",
    "first_bookings = first_bookings.withColumnRenamed('first_booking', 'first_booking_date')\n",
    "\n",
    "# create key to connect bookings\n",
    "first_bookings_0 = first_bookings.withColumn('fk_first_booking',F.concat(col(\"client\"), lit(\"\"), col(\"branch\")))\n",
    "\n",
    "\n",
    "# create new feature - here days since first booking\n",
    "timeFmt = \"dd-MM-yyyy\"\n",
    "today = \"02-07-2017\"\n",
    "first_bookings_0 = first_bookings_0.withColumn(\"today\",F.lit(today))\n",
    "first_bookings_0 = first_bookings_0.withColumn(\"first_booking_date_clean\",F.substring(first_bookings_0.first_booking_date, 0, 10))\n",
    "timeDiff = (F.unix_timestamp('today', format=timeFmt)-F.unix_timestamp('first_booking_date_clean', format=timeFmt))/24/3600\n",
    "first_bookings_0 = first_bookings_0.withColumn(\"customer_since\", timeDiff)\n",
    "\n",
    "\n",
    "# remove not-needed features\n",
    "first_bookings_0 = first_bookings_0.dropDuplicates()\n",
    "drop_list = ['client','branch','last_booking']\n",
    "first_bookings_EDA = first_bookings_0.select([column for column in first_bookings_0.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# save table to Hive for EDA\n",
    "first_bookings_EDA.write.mode(\"overwrite\").saveAsTable(\"coyote.first_bookings_EDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group first bookings by client, only client should have only one first booking date\n",
    "first_bookings_1 = first_bookings_0.groupBy(\"fk_first_booking\").agg({\"customer_since\": \"min\"})\n",
    "for name in first_bookings_1.schema.names: first_bookings_1 = first_bookings_1.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in first_bookings_1.schema.names: first_bookings_1 = first_bookings_1.withColumnRenamed(name, name.replace(')', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now join the dataframes\n",
    "clients_0 = clients_0.alias('clients_0')\n",
    "first_bookings_1 = first_bookings_1.alias('first_bookings_1')\n",
    "clients_1 = clients_0.join(first_bookings_1, clients_0.pk_client == first_bookings_1.fk_first_booking, \"right\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "With the above two chunk we loaded and cleaned all data from the __1st Booking__ table.<br>\n",
    "Furthermore relevant information was added to the forme client_0 table.<br>\n",
    "Output of these two steps is the table __clients_1__.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Pre-Process Data: Bookings Transfer <a id='5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data and clean columnnames\n",
    "booking_transfer = spark.read.csv(\"/datathon/Q2_Transfers_Bookings.txt\", header=True, sep=\"|\")\n",
    "for name in booking_transfer.schema.names:booking_transfer = booking_transfer.withColumnRenamed(name, name.lower())\n",
    "booking_transfer_0 = booking_transfer\n",
    "\n",
    "\n",
    "# key to match incidents / is unique booking_id at the same time | key to match clients\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"pk_incident\",F.concat(col(\"incoming_office\"), lit(\"\"), col(\"booking\"), lit(\"\"), col(\"service_order\")))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"fk_client\",F.concat(col(\"client\"), lit(\"\"), col(\"branch\")))\n",
    "\n",
    "\n",
    "# create new features (dates and simple counter)\n",
    "# cast to date function does not work # booking_transfer = booking_transfer.withColumn(\"booking_date_clean\", booking_transfer[\"booking_date_clean\"].cast(DateType()))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn('count', lit(1))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"booking_date_clean\",F.substring(booking_transfer_0.booking_date, 0, 10))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"service_date_clean\",F.substring(booking_transfer_0.service_date, 0, 10))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"booking_ahead\",F.datediff(booking_transfer_0.service_date_clean, booking_transfer_0.booking_date_clean))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"cancellation_date_clean\",F.substring(booking_transfer_0.cancellation_date, 0, 10))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"cancel_ahead\",F.datediff(booking_transfer_0.service_date_clean, booking_transfer_0.cancellation_date_clean))\n",
    "\n",
    "\n",
    "# reduce dimensionality when creating new features | named for EDA | numeric for modelling kMeans\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"interface_clean\",\n",
    "                                F.when(booking_transfer_0.interface_desc.like(\"%HOTEL%\"),\"Hotelbeds\").\\\n",
    "                                when(booking_transfer_0.interface_desc.like(\"%BED%\"),\"Bedsonline\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"interface_hotelbeds\",\n",
    "                                F.when(booking_transfer_0.interface_desc.like(\"%HOTEL%\"),\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"with_children\",\n",
    "                                F.when(booking_transfer_0.children>0,\"1\").\\\n",
    "                                when(booking_transfer_0.infants>0,\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"application_clean\",\n",
    "                                F.when(booking_transfer_0.application.like(\"%lution%\"),\"web\").\\\n",
    "                                when(booking_transfer_0.application.like(\"%xml%\"),\"xml\").\\\n",
    "                                when(booking_transfer_0.application.like(\"%app%\"),\"xml\").\\\n",
    "                                when(booking_transfer_0.application.like(\"%api%\"),\"api\").\\\n",
    "                                otherwise(\"irrelevant\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"application_web\",\n",
    "                                F.when(booking_transfer_0.application.like(\"%lution%\"),\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"application_api\",\n",
    "                                F.when(booking_transfer_0.application.like(\"%api%\"),\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"sales_method_clean\",\n",
    "                                F.when(booking_transfer_0.sales_method.like(\"%widget%\"),\"widget\").\\\n",
    "                                when(booking_transfer_0.application.like(\"%CROSS%\"),\"cross\").\\\n",
    "                                when(booking_transfer_0.application.like(\"%Cross%\"),\"cross\").\\\n",
    "                                when(booking_transfer_0.application.like(\"%OFF%\"),\"offline\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"master_service_type_clean\",\n",
    "                                F.when(booking_transfer_0.master_service_type_desc.like(\"%Shared%\"),\"shared\").\\\n",
    "                                when(booking_transfer_0.master_service_type_desc.like(\"%Private%\"),\"private\").\\\n",
    "                                otherwise(\"other\")) \n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"product_luxury\",\n",
    "                                F.when(booking_transfer_0.master_product_type_desc.like(\"%Luxury%\"),\"1\").\\\n",
    "                                when(booking_transfer_0.master_product_type_desc.like(\"%Premium%\"),\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_transfer_0 = booking_transfer_0.withColumn('international_booking',F.when(booking_transfer_0.destination_country==booking_transfer_0.country_market,lit(1)))\n",
    "\n",
    "\n",
    "# instiate  and rename features\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"ttv_eur\", booking_transfer_0[\"ttv_eur\"].cast(DoubleType()))\n",
    "booking_transfer_0 = booking_transfer_0.withColumn(\"pct_commission\", booking_transfer_0[\"pct_commission\"].cast(IntegerType()))\n",
    "booking_transfer_0 = booking_transfer_0.withColumnRenamed('destination_country_desc','country_destination')\n",
    "\n",
    "\n",
    "# run basic data quality checks (also in Tableau - iterative process) # remove bookings that cannot be matched to any client (according to documentation)\n",
    "# booking_transfer = booking_transfer.where(col(\"fk_client\").isNull()).count()\n",
    "booking_transfer_0 = booking_transfer_0.na.drop(subset=[\"fk_client\"])\n",
    "\n",
    "# remove not-needed features\n",
    "keep_list = ['fk_client','destination_country','interface_clean','country_destination','booking_date_clean','ttv_eur','booking_ahead','application_clean','']\n",
    "booking_transfer_EDA = booking_transfer_0.select([column for column in booking_transfer_0.columns if column in keep_list])\n",
    "\n",
    "\n",
    "# Store data to HIVE \n",
    "#booking_transfer_EDA.printSchema()\n",
    "#booking_transfer_EDA.select(\"master_product_type_desc\",\"luxury_pct\").show(100)\n",
    "booking_transfer_EDA.write.mode(\"overwrite\").saveAsTable(\"coyote.booking_transfer_EDA\")\n",
    "\n",
    "\n",
    "# Store data to booking_transfer_1\n",
    "# booking_transfer_1.printSchema()\n",
    "keep_list = ['international_booking','ttv_eur','pct_commission','pk_incident','fk_client','count','booking_ahead','interface_hotelbeds','with_children','application_web','application_api','product_luxury']\n",
    "booking_transfer_1 = booking_transfer_0.select([column for column in booking_transfer_0.columns if column in keep_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "The above chunck has two outputs:  \n",
    "1. Hive Table __booking_transfer_EDA__  for EDA in Tableau <br>\n",
    "2. Dataframe __booking_transfer_1__ for modelling\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load, Pre-Process and Incidents | Join  Bookings <a id='6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and clean column names\n",
    "incidents = spark.read.csv(\"/datathon/IE_Challenge_4.IncidentsClaims v2.txt\", header=True, sep=\"#\")\n",
    "for name in incidents.schema.names: incidents = incidents.withColumnRenamed(name, name.lower())\n",
    "for name in incidents.schema.names: incidents = incidents.withColumnRenamed(name, name.replace(' ', '_'))\n",
    "\n",
    "\n",
    "# create key to connect bookings and add counter\n",
    "incidents_0 = incidents.withColumn('fk_incidents',F.concat(col(\"office_number\"), lit(\"\"), col(\"booking_number\"), lit(\"\"), col(\"service_id\")))\n",
    "incidents_0 = incidents_0.withColumn('count', lit(1))\n",
    "\n",
    "\n",
    "# remove pre-travel complaints since they are not always negativ / rename features\n",
    "incidents_0 = incidents_0.filter(~col('life_cycle').isin([\"Pre Arrival\"]))\n",
    "incidents_0 = incidents_0.withColumnRenamed('date/time_opened','incident_date')\n",
    "\n",
    "\n",
    "# remove not-needed features\n",
    "keep_list = ['fk_incidents','date_time','life_cycle','service_type','count']\n",
    "incidents_EDA = incidents_0.select([column for column in incidents_0.columns if column in keep_list])\n",
    "\n",
    "\n",
    "# save to hive\n",
    "# incidents.printSchema()\n",
    "incidents_EDA.write.mode(\"overwrite\").saveAsTable(\"coyote.incidents_EDA\")\n",
    "\n",
    "\n",
    "# Store data to incidents_1 (this df will be added to the booking df and be an input for segmentation)\n",
    "keep_list = ['fk_incidents','date_time','life_cycle','service_type','count']\n",
    "incidents_1 = incidents_0.select([column for column in incidents_0.columns if column in keep_list])\n",
    "incidents_1 = incidents_1.groupBy(\"fk_incidents\").agg({\"count\": \"sum\"})\n",
    "incidents_1 = incidents_1.withColumnRenamed('sum(count)','no_incidents')\n",
    "incidents_1 = incidents_1.na.drop(subset=[\"fk_incidents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join booking_transfer_1 and incidents\n",
    "incidents_1 = incidents_1.alias('incidents_1')\n",
    "booking_transfer_1 = booking_transfer_1.alias('booking_transfer_1')\n",
    "booking_transfer_2 = booking_transfer_1.join(incidents_1, booking_transfer_0.pk_incident == incidents_1.fk_incidents, \"left\")\n",
    "\n",
    "# run basic checks\n",
    "# booking_transfer_1.filter(col('no_incidents').isin([\"1\",\"2\",\"3\"])).select(\"fk_client\",\"pk_incident\",\"fk_incidents\",\"no_incidents\").groupBy(\"fk_client\",\"fk_incidents\").agg({\"no_incidents\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "Incidents and Claims was loaded and cleaned. In some cases, there was more than one incident per booking. If the case, number of incidents were aggregated. Furthermore only negative incidents were considered, since number of incidents shall serve as KPI for the customer segmentation and indicate the effort for individual handling. <br>\n",
    "<br>\n",
    "__Number of incidents__ was added to  table booking_transfer_1.<br>\n",
    "Output is dataframe:__booking_transfer_2__.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Aggregate Bookings_Transfer and Join: Clients_1 <a id='7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add prefix that states origin and helps better understand data\n",
    "booking_transfer_3 = booking_transfer_2.groupby(\"fk_client\").agg({\"with_children\":\"sum\", \n",
    "                                                                  \"pct_commission\":\"avg\", \n",
    "                                                                  \"booking_ahead\":\"avg\", \n",
    "                                                                  \"ttv_eur\":\"sum\",\n",
    "                                                                  \"no_incidents\":\"sum\",\n",
    "                                                                  \"application_web\":\"sum\",\n",
    "                                                                  \"application_api\":\"sum\",\n",
    "                                                                  \"product_luxury\":\"sum\",\n",
    "                                                                  \"count\":\"sum\",\n",
    "                                                                  \"international_booking\":\"sum\"\n",
    "                                                                  })\n",
    "\n",
    "for name in booking_transfer_3.schema.names: booking_transfer_3 = booking_transfer_3.withColumnRenamed(name, \"booking_transfer_\"+name)\n",
    "for name in booking_transfer_3.schema.names: booking_transfer_3 = booking_transfer_3.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in booking_transfer_3.schema.names: booking_transfer_3 = booking_transfer_3.withColumnRenamed(name, name.replace(')', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join clients and booking_transfer_3\n",
    "booking_transfer_3 = booking_transfer_3.alias('booking_transfer_3')\n",
    "clients_1 = clients_1.alias('clients_1')\n",
    "clients_2 = clients_1.join(booking_transfer_3, clients_1.pk_client == booking_transfer_3.booking_transfer_fk_client, \"right\")\n",
    "\n",
    "# rename columns\n",
    "for name in clients_2.schema.names: clients_2 = clients_2.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in clients_2.schema.names: clients_2 = clients_2.withColumnRenamed(name, name.replace(')', ''))\n",
    "\n",
    "#cross-check positive - total ttv didnt change\n",
    "#clients_2.agg({\"sum_ttv_eur_trans\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "In the two above chunk __booking_transfer_2__ was grouped by client (= __booking_transfer_3__) and then merged with the client dataframe.<br>\n",
    "Output is the dataframe __clients_2__.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and Pre-Process Data: Bookings Activities <a id='8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data # clean column names\n",
    "booking_act = spark.read.csv(\"/datathon/Q1_Activities_Bookings.txt\", header=True, sep=\"|\")\n",
    "for name in booking_act.schema.names: booking_act = booking_act.withColumnRenamed(name, name.lower())\n",
    "for name in booking_act.schema.names: booking_act = booking_act.withColumnRenamed(name, name.replace(' ', '_'))\n",
    "booking_act = booking_act.withColumnRenamed('destination_country_desc','country_destination')\n",
    "\n",
    "\n",
    "# key to match incidents / is unique booking_id at the same time | key to match clients\n",
    "# booking_act_0 = booking_act_0.withColumn(\"fk_promo\", substring(booking_act_0[\"service\"],-4,4)) #substring to join promotions and tickets table\n",
    "# booking_act_0 = booking_act_0.withColumn(\"fk_bundles\", substring(booking_act_0[\"service\"],-4,4)) #substring to join promotions and tickets table\n",
    "booking_act_0 = booking_act.withColumn(\"fk_client\",F.concat(col(\"client\"), lit(\"\"), col(\"branch\")))\n",
    "booking_act_0 = booking_act_0.withColumn(\"pk_incident\",F.concat(col(\"incoming_office\"), lit(\"\"), col(\"booking\"), lit(\"\"), col(\"service_order\")))\n",
    "booking_act_0 = booking_act_0.withColumnRenamed('content_factsheet_code','fk_product')\n",
    "\n",
    "# create new features (dates and simple counter)\n",
    "#booking_transfer = booking_transfer.withColumn(\"booking_date_clean\", booking_transfer[\"booking_date_clean\"].cast(DateType()))\n",
    "booking_act_0 = booking_act_0.withColumn('count', lit(1))\n",
    "booking_act_0 = booking_act_0.withColumn(\"booking_date_clean\",F.substring(booking_act_0.booking_date, 0, 10))\n",
    "booking_act_0 = booking_act_0.withColumn(\"service_date_clean\",F.substring(booking_act_0.service_date_from, 0, 10))\n",
    "booking_act_0 = booking_act_0.withColumn(\"booking_ahead\",F.datediff(booking_act_0.service_date_clean, booking_act_0.booking_date_clean))\n",
    "booking_act_0 = booking_act_0.withColumn(\"cancellation_date_clean\",F.substring(booking_act_0.cancellation_date, 0, 10))\n",
    "booking_act_0 = booking_act_0.withColumn(\"cancel_ahead\",F.datediff(booking_act_0.service_date_clean, booking_act_0.cancellation_date_clean))\n",
    "booking_act_0 = booking_act_0.withColumn('international_booking',F.when(booking_act_0.destination_country==booking_act_0.country_market,lit(1)))\n",
    "\n",
    "\n",
    "# reduce dimensionality when creating new features | named for EDA | numeric for modelling kMeans\n",
    "booking_act_0 = booking_act_0.withColumn(\"interface_clean\",\n",
    "                                F.when(booking_act_0.interface_desc.like(\"%HOTEL%\"),\"Hotelbeds\").\\\n",
    "                                when(booking_act_0.interface_desc.like(\"%BED%\"),\"Bedsonline\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"interface_hotelbeds\",\n",
    "                                F.when(booking_act_0.interface_desc.like(\"%HOTEL%\"),\"1\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"children_clean\",\n",
    "                                F.when(booking_act_0.children>0,\"YES\").\\\n",
    "                                when(booking_act_0.infants>0,\"YES\").\\\n",
    "                                otherwise(\"NO\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"with_children\",\n",
    "                                F.when(booking_act_0.children>0,\"1\").\\\n",
    "                                when(booking_act_0.infants>0,\"1\").\\\n",
    "                                otherwise(\"NO\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"application_clean\",\n",
    "                                F.when(booking_act_0.application.like(\"%lution%\"),\"web\").\\\n",
    "                                when(booking_act_0.application.like(\"%xml%\"),\"xml\").\\\n",
    "                                when(booking_act_0.application.like(\"%app%\"),\"xml\").\\\n",
    "                                when(booking_act_0.application.like(\"%api%\"),\"api\").\\\n",
    "                                otherwise(\"irrelevant\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"application_web\",\n",
    "                                F.when(booking_act_0.application.like(\"%lution%\"),\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"application_api\",\n",
    "                                F.when(booking_act_0.application.like(\"%api\"),\"1\").\\\n",
    "                                otherwise(\"0\"))\n",
    "\n",
    "booking_act_0 = booking_act_0.withColumn(\"sales_method_clean\",\n",
    "                                F.when(booking_act_0.sales_method.like(\"%widget%\"),\"widget\").\\\n",
    "                                when(booking_act_0.application.like(\"%CROSS%\"),\"cross\").\\\n",
    "                                when(booking_act_0.application.like(\"%Cross%\"),\"cross\").\\\n",
    "                                when(booking_act_0.application.like(\"%OFF%\"),\"offline\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "\n",
    "# instantiate data type #drop bookings without assigned customer\n",
    "booking_act_0 = booking_act_0.na.drop(subset=[\"fk_client\"])\n",
    "booking_act_0 = booking_act_0.withColumn(\"ttv_eur\", booking_act_0[\"ttv_eur\"].cast(FloatType()))\n",
    "booking_act_0 = booking_act_0.withColumn(\"pct_commission\", booking_act_0[\"pct_commission\"].cast(FloatType()))\n",
    "#booking_act = booking_act.withColumn('booking_date', func(col('booking_date')))\n",
    "#booking_act = booking_act.withColumn('cancellation_date', func(col('cancellation_date')))\n",
    "#booking_act = booking_act.withColumn('service_date_from', func(col('service_date_from')))\n",
    "#booking_act = booking_act.withColumn('service_date_to', func(col('service_date_to')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "Boooking Acivities is loaded and cleaned. Output is:  \n",
    "1. Output: __booking_act_0__ -  intermediate for further processing before EDA and Modelling\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Join Incidents, Promotional Deals and Bundles to booking_act <a id='9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: Incidents are already cleaned\n",
    "booking_act_1 = booking_act_0.join(incidents_1, booking_act_0.pk_incident == incidents_1.fk_incidents, \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: based on key fk_promo promotions are supposed to be joined - since there are very few hits the field is redundant\n",
    "#check = booking_act.withColumn(\"fk_promo\", substring(booking_act[\"service\"],-4,4)).select(\"fk_promo\")\n",
    "#check = check.withColumn('count', lit(1))\n",
    "#check.filter(col('fk_promo').isin([\"OFEB\",\"OFNR\",\"OFFC\", \"OFLD\"])).count()\n",
    "#check.filter(col('fk_promo').isin([\"OFEB\",\"OFNR\",\"OFFC\", \"OFLD\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: based on key fk_promo bundles are supposed to be joined - since there are very few hits the field is redundant\n",
    "#bundles = spark.read.csv(\"/datathon/IE_Challenge_7.TicketBundles_service code prefix v2.txt\", header=True, sep=\"#\")\n",
    "#codes_promo = bundles.select(\"Code\").rdd.flatMap(lambda x: x).collect()\n",
    "#check.filter(col('fk_promo').isin(codes_promo).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Incident data has been added.  \n",
    "Output: __booking_act_1__.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load, Pre-Process and Join Booking_Act: Products <a id='10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data and clean headers\n",
    "products = spark.read.csv(\"/datathon/IE_Challenge_2.ActivitiesContentFactsheet v2.txt\", header=True, sep=\"#\")\n",
    "products = products.withColumnRenamed('Factsheet ID','pk_product').withColumnRenamed('Segmentation - duration','duration')\n",
    "\n",
    "# introduce dummy features for model / can be used for EDA as well (convert to dimension)\n",
    "products_0 = products.withColumn('target_couples',F.when(products.Segmentation.like(\"%Couples%\"),lit(1)))\n",
    "products_0 = products_0.withColumn('target_families',F.when(products_0.Segmentation.like(\"%Fami%\"),lit(1)))\n",
    "products_0 = products_0.withColumn('target_seniors',F.when(products_0.Segmentation.like(\"%Sen%\"),lit(1)))\n",
    "products_0 = products_0.withColumn('target_youth',F.when(products_0.Segmentation.like(\"%You%\"),lit(1)))\n",
    "products_0 = products_0.withColumn('full_day',F.when(products_0.duration.like(\"%Full day%\"),lit(1)))\n",
    "\n",
    "drop_list = ['Concepts','duration','Segmentation']\n",
    "products = products.select([column for column in products.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join df booking_act_1\n",
    "products_0 = products_0.alias('products_0')\n",
    "booking_act_1 = booking_act_1.alias('booking_act_1')\n",
    "booking_act_2 = booking_act_1.join(products_0, booking_act_1.fk_product == products_0.pk_product, \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Products were added.  \n",
    "Output: __booking_act_2__. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Store booking_act_2 to HIVE <a id='11'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove not-needed features and save to HIVE for EDA\n",
    "keep_list = ['fk_client','destination_country','interface_clean','country_destination','booking_date_clean','ttv_eur','booking_ahead','application_clean','full_day','target_families']\n",
    "booking_act_EDA = booking_act_2.select([column for column in booking_act_2.columns if column in keep_list])\n",
    "booking_act_EDA.write.mode(\"overwrite\").saveAsTable(\"coyote.booking_act_EDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "DF stored to HIVE for further processing in Tableau. Relevent features were selected.  \n",
    "Output: __booking_act_EDA__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Aggregate Bookings_Activities and Join: Clients_2 <a id='12'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add prefix that states origin and helps better understand data\n",
    "booking_act_3 = booking_act_2.groupby(\"fk_client\").agg({\"ttv_eur\":\"sum\", \n",
    "                                                                  \"pct_commission\":\"avg\", \n",
    "                                                                  \"with_children\":\"sum\", \n",
    "                                                                  \"no_incidents\":\"sum\",\n",
    "                                                                  \"booking_ahead\":\"avg\",\n",
    "                                                                  \"target_couples\":\"sum\",\n",
    "                                                                  \"target_families\":\"sum\",\n",
    "                                                                  \"target_seniors\":\"sum\",\n",
    "                                                                  \"target_youth\":\"sum\",\n",
    "                                                                  \"full_day\":\"sum\",\n",
    "                                                                  \"application_web\":\"sum\",\n",
    "                                                                  \"application_api\":\"sum\",\n",
    "                                                                  \"international_booking\":\"sum\",\n",
    "                                                                  \"count\":\"sum\"\n",
    "                                                                  })\n",
    "\n",
    "for name in booking_act_3.schema.names: booking_act_3 = booking_act_3.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in booking_act_3.schema.names: booking_act_3 = booking_act_3.withColumnRenamed(name, name.replace(')', ''))\n",
    "for name in booking_act_3.schema.names: booking_act_3 = booking_act_3.withColumnRenamed(name, \"booking_act_\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join clients and booking_act_3\n",
    "# full join since we dont want loose any sales from transfers (if customer is in list, he has at least transfer sales)\n",
    "booking_act_3 = booking_act_3.alias('booking_act_3')\n",
    "clients_2 = clients_2.alias('clients_2')\n",
    "clients_3 = clients_2.join(booking_act_3, clients_2.pk_client == booking_act_3.booking_act_fk_client, \"full\")\n",
    "\n",
    "for name in clients_3.schema.names: clients_3 = clients_3.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in clients_3.schema.names: clients_3 = clients_3.withColumnRenamed(name, name.replace(')', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### cross-check positive - total ttv didnt change\n",
    "#clients_3.agg({\"sum_ttv_eur_trans\": \"sum\"}).show()\n",
    "#booking_act.agg({\"ttv_eur\": \"sum\"}).show()\n",
    "#clients_3.agg({\"sum_ttv_eur_act\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Booking Activities was added to clients_2 <br>\n",
    "Output: __clients_3__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Load, Pre-Process and Join Clients_3: Demand Transfer <a id='13'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data # add column names\n",
    "demand_transfer = spark.read.csv(\"/datathon/IE_Challenge_0.Transfers_Demand.txt\", header=False, sep=\"|\")\n",
    "demand_transfer = demand_transfer.withColumnRenamed('_c0','request_date').withColumnRenamed('_c1','request_hour').withColumnRenamed('_c2','customer_id').withColumnRenamed('_c3','customer_branch').withColumnRenamed('_c4','des_interface').withColumnRenamed('_c5','booking_gate'). withColumnRenamed('_c6','pick_up_date'). withColumnRenamed('_c7','pick_up_hour'). withColumnRenamed('_c8','status'). withColumnRenamed('_c9','pick_up_type'). withColumnRenamed('_c10','pick_up_hotel'). withColumnRenamed('_c11','pick_up_terminal'). withColumnRenamed('_c12','pick_up_zone'). withColumnRenamed('_c13','drop_off_type'). withColumnRenamed('_c14','drop_off_hotel'). withColumnRenamed('_c15','drop_off_terminal'). withColumnRenamed('_c16','drop_off_zone'). withColumnRenamed('_c17','pick_up_zone2'). withColumnRenamed('_c18','drop_off_zone2'). withColumnRenamed('_c19','num_adults'). withColumnRenamed('_c20','num_children'). withColumnRenamed('_c21','num_requests'). withColumnRenamed('_c22','avg_products_returned'). withColumnRenamed('_c23','sales_origin')\n",
    "\n",
    "# create primary key: incident / is unique booking_id at the same time\n",
    "demand_transfer_0 = demand_transfer.withColumn(\"fk_client\",F.concat(col(\"customer_id\"), lit(\"\"), col(\"customer_branch\")))\n",
    "\n",
    "\n",
    "# clean features\n",
    "demand_transfer_0 = demand_transfer_0.withColumn(\"interface_clean\",\n",
    "                                F.when(demand_transfer_0.des_interface.like(\"%HOTEL%\"),\"Hotelbeds\").\\\n",
    "                                when(demand_transfer_0.des_interface.like(\"%BED%\"),\"Bedsonline\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "#demand_act.select('booking_gate').show(1000) # OK - features can be translated as for bookings\n",
    "demand_transfer_0 = demand_transfer_0.withColumn(\"application_clean\",\n",
    "                                F.when(demand_transfer_0.booking_gate.like(\"%lution%\"),\"web\").\\\n",
    "                                when(demand_transfer_0.booking_gate.like(\"%xml%\"),\"xml\").\\\n",
    "                                when(demand_transfer_0.booking_gate.like(\"%app%\"),\"xml\").\\\n",
    "                                when(demand_transfer_0.booking_gate.like(\"%ACT%\"),\"api\").\\\n",
    "                                otherwise(\"irrelevant\"))\n",
    "\n",
    "# transform data type\n",
    "#demand_act = demand_act.withColumn('request_date', func(col('request_date')))\n",
    "#demand_transfer = demand_transfer.withColumn('request_date', func(col('request_date')))\n",
    "demand_transfer_0 = demand_transfer_0.withColumn(\"avg_products_returned\", demand_transfer_0[\"avg_products_returned\"].cast(DoubleType()))\n",
    "demand_transfer_0 = demand_transfer_0.withColumn(\"num_requests\", demand_transfer_0[\"num_requests\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select eda reelevant features and save to HIVE\n",
    "keep_list = ['fk_client','request_date','interface_clean','application_clean','status','num_requests','avg_products_returned']\n",
    "demand_transfer_eda = demand_transfer_0.select([column for column in demand_transfer_0.columns if column in keep_list])\n",
    "demand_transfer_eda.write.mode(\"overwrite\").saveAsTable(\"coyote.demand_transfer_eda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will consider num_requests and average products returned for the segmentation(booking gate is included from actual bookings) \n",
    "demand_transfer_1 = demand_transfer_0.groupBy(\"fk_client\").agg({\"num_requests\":\"sum\",\"avg_products_returned\":\"avg\"})\n",
    "for name in demand_transfer_1.schema.names: demand_transfer_1 = demand_transfer_1.withColumnRenamed(name, \"demand_transfer_\"+name)\n",
    "    \n",
    "# add information from demand_transfer to clients table\n",
    "clients_4 = clients_3.join(demand_transfer_1, clients_3.pk_client == demand_transfer_1.demand_transfer_fk_client, \"left\")\n",
    "for name in clients_4.schema.names: clients_4 = clients_4.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in clients_4.schema.names: clients_4 = clients_4.withColumnRenamed(name, name.replace(')', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Demand_Transfer was added to clients_3 <br>\n",
    "Output: __clients_4__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Load, Pre-Process and Join Clients_4: Demand Activities <a id='14'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data # add columnnames\n",
    "demand_act = spark.read.csv(\"/datathon/IE_Challenge_0.Activities_Demand.txt\", header=False, sep=\"|\")\n",
    "demand_act = demand_act.withColumnRenamed('_c0','request_date').withColumnRenamed('_c1','request_hour').withColumnRenamed('_c2','customer_id').withColumnRenamed('_c3','customer_branch').withColumnRenamed('_c4','des_interface').withColumnRenamed('_c5','booking_gate').withColumnRenamed('_c6','valid_from').withColumnRenamed('_c7','valid_to').withColumnRenamed('_c8','status').withColumnRenamed('_c9','pick_up_zone').withColumnRenamed('_c10','pick_up_hotel').withColumnRenamed('_c11','num_adult').withColumnRenamed('_c12','num_children').withColumnRenamed('_c13','num_requests').withColumnRenamed('_c14','avg_products_returned').withColumnRenamed('_c15','sales_origin')\n",
    "\n",
    "\n",
    "# create primary key: incident / is unique booking_id at the same time\n",
    "demand_act_0 = demand_act.withColumn(\"fk_client\",F.concat(col(\"customer_id\"), lit(\"\"), col(\"customer_branch\")))\n",
    "\n",
    "\n",
    "### clean features\n",
    "demand_act_0 = demand_act_0.withColumn(\"interface_clean\",\n",
    "                                F.when(demand_act_0.des_interface.like(\"%HOTEL%\"),\"Hotelbeds\").\\\n",
    "                                when(demand_act_0.des_interface.like(\"%BED%\"),\"Bedsonline\").\\\n",
    "                                otherwise(\"other\"))\n",
    "\n",
    "#demand_act.select('booking_gate').show(1000) # OK - features can be translated as for bookings\n",
    "demand_act_0 = demand_act_0.withColumn(\"application_clean\",\n",
    "                                F.when(demand_act_0.booking_gate.like(\"%lution%\"),\"web\").\\\n",
    "                                when(demand_act_0.booking_gate.like(\"%xml%\"),\"xml\").\\\n",
    "                                when(demand_act_0.booking_gate.like(\"%app%\"),\"xml\").\\\n",
    "                                when(demand_act_0.booking_gate.like(\"%ACT%\"),\"api\").\\\n",
    "                                otherwise(\"irrelevant\"))\n",
    "\n",
    "# transform data type\n",
    "#demand_act = demand_act.withColumn('request_date', func(col('request_date')))\n",
    "demand_act_0 = demand_act_0.withColumn(\"avg_products_returned\", demand_act_0[\"avg_products_returned\"].cast(DoubleType()))\n",
    "demand_act_0 = demand_act_0.withColumn(\"num_requests\", demand_act_0[\"num_requests\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select eda reelevant features and save to HIVE\n",
    "keep_list_demand_act = ['fk_client','request_date','interface_clean','application_clean','status','num_requests','avg_products_returned']\n",
    "demand_act_eda = demand_act_0.select([column for column in demand_act_0.columns if column in keep_list_demand_act])\n",
    "demand_act_eda.write.mode(\"overwrite\").saveAsTable(\"coyote.demand_act_eda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will consider num_requests and average products returned for the segmentation(booking gate is included from actual bookings) \n",
    "demand_act_1 = demand_transfer_0.groupBy(\"fk_client\").agg({\"num_requests\":\"sum\",\"avg_products_returned\":\"avg\"})\n",
    "for name in demand_act_1.schema.names: demand_act_1 = demand_act_1.withColumnRenamed(name, \"demand_act_\"+name)\n",
    "\n",
    "# add information from demand_transfer to clients table\n",
    "clients_5 = clients_4.join(demand_act_1, clients_4.pk_client == demand_act_1.demand_act_fk_client, \"left\")\n",
    "for name in clients_5.schema.names: clients_5 = clients_5.withColumnRenamed(name, name.replace('(', '_'))\n",
    "for name in clients_5.schema.names: clients_5 = clients_5.withColumnRenamed(name, name.replace(')', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Booking Activities was added to clients_4 <br>\n",
    "Output: __clients_5__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Consolidate Data for Model <a id='15'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate features - for efficient processing / numeric to run alg. calculations\n",
    "clients_final = clients_5\n",
    "clients_final = clients_final.withColumn(\"min_customer_since\", clients_final[\"min_customer_since\"].cast(IntegerType()))\n",
    "\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_count\", clients_final[\"booking_transfer_sum_count\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_ttv_eur\", clients_final[\"booking_transfer_sum_ttv_eur\"].cast(FloatType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_product_luxury\", clients_final[\"booking_transfer_sum_product_luxury\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_avg_pct_commission\", clients_final[\"booking_transfer_avg_pct_commission\"].cast(FloatType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_with_children\", clients_final[\"booking_transfer_sum_with_children\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_avg_booking_ahead\", clients_final[\"booking_transfer_avg_booking_ahead\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_no_incidents\", clients_final[\"booking_transfer_sum_no_incidents\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_application_api\", clients_final[\"booking_transfer_sum_application_api\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_transfer_sum_application_web\", clients_final[\"booking_transfer_sum_application_web\"].cast(IntegerType()))\n",
    "\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_count\", clients_final[\"booking_act_sum_count\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_ttv_eur\", clients_final[\"booking_act_sum_ttv_eur\"].cast(FloatType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_avg_pct_commission\", clients_final[\"booking_act_avg_pct_commission\"].cast(FloatType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_with_children\", clients_final[\"booking_act_sum_with_children\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_avg_booking_ahead\", clients_final[\"booking_act_avg_booking_ahead\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_target_youth\", clients_final[\"booking_act_sum_target_youth\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_international_booking\", clients_final[\"booking_act_sum_international_booking\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_target_seniors\", clients_final[\"booking_act_sum_target_seniors\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_target_families\", clients_final[\"booking_act_sum_target_families\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_full_day\", clients_final[\"booking_act_sum_full_day\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_no_incidents\", clients_final[\"booking_act_sum_no_incidents\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"booking_act_sum_target_couples\", clients_final[\"booking_act_sum_target_couples\"].cast(IntegerType()))\n",
    "\n",
    "clients_final = clients_final.withColumn(\"demand_transfer_sum_num_requests\", clients_final[\"demand_transfer_sum_num_requests\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"demand_transfer_avg_avg_products_returned\", clients_final[\"demand_transfer_avg_avg_products_returned\"].cast(FloatType()))\n",
    "\n",
    "clients_final = clients_final.withColumn(\"demand_act_sum_num_requests\", clients_final[\"demand_act_sum_num_requests\"].cast(IntegerType()))\n",
    "clients_final = clients_final.withColumn(\"demand_act_avg_avg_products_returned\", clients_final[\"demand_act_avg_avg_products_returned\"].cast(FloatType()))\n",
    "\n",
    "clients_final = clients_final.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create totals from ttv and count plus its ratios\n",
    "clients_final = clients_final.withColumn(\"total_ttv\",clients_final.booking_transfer_sum_ttv_eur + clients_final.booking_act_sum_ttv_eur)\n",
    "clients_final = clients_final.withColumn(\"share_transfer_ttv\",clients_final.booking_transfer_sum_ttv_eur / clients_final.total_ttv)\n",
    "clients_final = clients_final.withColumn(\"total_bookings_count\",clients_final.booking_transfer_sum_count + clients_final.booking_act_sum_count)\n",
    "clients_final = clients_final.withColumn(\"bookings_per_day\",clients_final.total_bookings_count / clients_final.min_customer_since )\n",
    "\n",
    "# create ratio: average_ttv per booking\n",
    "clients_final = clients_final.withColumn(\"ttv_per_booking\",clients_final.total_ttv / clients_final.total_bookings_count)\n",
    "\n",
    "# create total and ratio: number of incidents\n",
    "clients_final = clients_final.withColumn(\"total_incidents_count\",clients_final.booking_transfer_sum_no_incidents + clients_final.booking_act_sum_no_incidents)\n",
    "#clients_final = clients_final.withColumn(\"incidents_per_day\",clients_final.total_incidents_count / clients_final.min_customer_since)\n",
    "clients_final = clients_final.withColumn(\"incidents_per_booking\",clients_final.total_incidents_count / clients_final.total_bookings_count)\n",
    "\n",
    "\n",
    "# sum up number of requests, in order to identify highly demanding customers, build ratio: requests / bookings\n",
    "clients_final = clients_final.withColumn(\"total_requests_count\",clients_final.demand_transfer_sum_num_requests + clients_final.demand_act_sum_num_requests)\n",
    "clients_final = clients_final.withColumn(\"requests_per_booking\",clients_final.total_requests_count / clients_final.total_bookings_count)\n",
    "clients_final = clients_final.withColumn(\"requests_since_first_booking\",clients_final.total_requests_count / clients_final.min_customer_since)\n",
    "\n",
    "# sum up booking channel and build ration per total number of bookings, remove subcategories\n",
    "clients_final = clients_final.withColumn(\"share_booking_api\",(clients_final.booking_transfer_sum_application_api + clients_final.booking_act_sum_application_api) / clients_final.total_bookings_count) \n",
    "clients_final = clients_final.withColumn(\"share_booking_web\",(clients_final.booking_transfer_sum_application_web + clients_final.booking_act_sum_application_web) / clients_final.total_bookings_count) \n",
    "\n",
    "# sum up total international bookings and build ration per total number of bookings, remove subcategories\n",
    "clients_final = clients_final.withColumn(\"share_international_booking\",(clients_final.booking_act_sum_international_booking + clients_final.booking_transfer_sum_international_booking) / clients_final.total_bookings_count) \n",
    "\n",
    "# build ratio of total bookings (used for activities only)\n",
    "clients_final = clients_final.withColumn(\"share_booking_children\",clients_final.booking_act_sum_with_children / clients_final.total_bookings_count)\n",
    "clients_final = clients_final.withColumn(\"share_booking_full_day\",clients_final.booking_act_sum_full_day / clients_final.total_bookings_count)\n",
    "clients_final = clients_final.withColumn(\"share_booking_target_couple\",clients_final.booking_act_sum_target_couples / clients_final.total_bookings_count)\n",
    "clients_final = clients_final.withColumn(\"share_booking_target_families\",clients_final.booking_act_sum_target_families / clients_final.total_bookings_count)\n",
    "clients_final = clients_final.withColumn(\"share_booking_target_youth\",clients_final.booking_act_sum_target_youth / clients_final.total_bookings_count)\n",
    "clients_final = clients_final.withColumn(\"share_booking_target_seniors\",clients_final.booking_act_sum_target_seniors / clients_final.total_bookings_count)\n",
    "\n",
    "\n",
    "# build ratio of total bookings (used for transfer only)\n",
    "clients_final = clients_final.withColumn(\"share_booking_transfer_luxury\",clients_final.booking_transfer_sum_product_luxury / clients_final.total_bookings_count)\n",
    "\n",
    "\n",
    "# dont average, danger of a client only booking one category\n",
    "# |-- booking_act_avg_booking_ahead: integer (nullable = true)\n",
    "# |-- booking_transfer_avg_booking_ahead: integer (nullable = true)\n",
    "\n",
    "# dont average, different business   \n",
    "# |-- booking_transfer_avg_pct_commission: float (nullable = false)\n",
    "# |-- booking_act_avg_pct_commission: float (nullable = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove features (in an iterative process with EDA in Tableau)\n",
    "drop_list = ['booking_transfer_sum_no_incidents','booking_act_sum_no_incidents',\n",
    "             'demand_act_sum_num_requests','demand_transfer_sum_num_requests',\n",
    "             'demand_transfer_avg_avg_products_returned','demand_act_avg_avg_products_returned',\n",
    "             'booking_act_sum_with_children','booking_act_sum_full_day',\n",
    "             'booking_transfer_sum_application_api','booking_act_sum_application_api',\n",
    "             'booking_transfer_sum_application_web','booking_act_sum_application_web',                \n",
    "             'booking_act_sum_international_booking','booking_act_sum_international_booking',\n",
    "             'booking_act_sum_target_couples','booking_act_sum_target_families',\n",
    "             'booking_act_sum_target_youth','booking_act_sum_target_seniors',\n",
    "             'booking_transfer_sum_with_children','booking_transfer_sum_product_luxury',\n",
    "             'booking_act_fk_client','demand_transfer_fk_client','demand_act_fk_client']\n",
    "             \n",
    "clients_final = clients_final.select([column for column in clients_final.columns if column not in drop_list])           \n",
    "clients_final.write.mode(\"overwrite\").saveAsTable(\"coyote.clients_final\")\n",
    "#clients_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Features were selected, consolidated and combined <br>\n",
    "Output: __clients_final__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. High-level validation <a id='16'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## all relevant ttv from booking_transfer is covered\n",
    "#clients_5.select(\"booking_transfer_sum_ttv_eur\").agg({\"booking_transfer_sum_ttv_eur\":\"sum\"}).show()\n",
    "#booking_transfer_0.select(\"ttv_eur\").agg({\"ttv_eur\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## all relevant ttv from booking_act is covered\n",
    "#clients_5.select(\"booking_act_sum_ttv_eur\").agg({\"booking_act_sum_ttv_eur\":\"sum\"}).show()\n",
    "#booking_act_0.select(\"ttv_eur\").agg({\"ttv_eur\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## final ttv equals sum of transfer and activity\n",
    "#clients_final.select(\"total_ttv\").agg({\"total_ttv\":\"sum\"}).show()\n",
    "#clients_final.select(\"booking_transfer_sum_ttv_eur\",\"booking_act_sum_ttv_eur\",\"total_ttv\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Everything alright, other checks run within code </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Code - Backlog <a id='17'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"> \n",
    "Please skip reading, for my personal try and error only. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Within this chunk total TTV per customer by Activity vs. Transfer is calculates\n",
    "# in a second step client data is joined\n",
    "\n",
    "#clients = clients.alias('clients')\n",
    "#total_transfer = booking_transfer.select(\"CLIENT\",\"TTV_TRANS\").groupby(\"CLIENT\").agg({\"TTV_TRANS\":\"sum\"}).withColumnRenamed('sum(TTV_TRANS)','total_trans')\n",
    "#total_act = booking_act.select(\"CLIENT\",\"TTV_ACT\").groupby(\"CLIENT\").agg({\"TTV_ACT\":\"sum\"}).withColumnRenamed('sum(TTV_ACT)','total_act')                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#act_totalTTV = booking_act.select(\"CLIENT\",\"TTV\").groupBy(\"CLIENT\").agg({\"TTV\":\"sum\"}).orderBy('sum(TTV)', ascending=False).toPandas()\n",
    "#act_totalTTV.boxplot(column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None)\n",
    "\n",
    "#act_cumTTV = act_totalTTV.cumsum(axis=None, dtype=None, out=None, skipna=True)\n",
    "#act_cumTTV.plot(x=None, y=None, kind='line', ax=None, subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Duration = products.select(\"duration_clean\").groupby(\"duration_clean\").agg({\"duration_clean\":\"count\"}).toPandas()\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "\n",
    "#objects = ['Other', 'Half Day', 'Full Day']\n",
    "#x = list(range(len(objects)))\n",
    "#y = Duration['count(duration_clean)'].tolist()\n",
    "\n",
    "#plt.bar(x, y, align='center', alpha=0.5)\n",
    "#plt.xticks(y_pos, objects)\n",
    "#plt.ylabel('Number of Bookings')\n",
    "#plt.title('Distribution of Duration')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#demand_act.registerTempTable(\"demand_act\")\n",
    "#spark.sql(\"Select mean(num_children) from demand_act\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#bycountry = booking_act.groupby(\"COUNTRY_MARKET_DESC\").agg({\"PCT_COMMISSION\":\"mean\"}).toPandas().sort_values(\"avg(PCT_COMMISSION)\", axis=0, ascending=False).head(5)\n",
    "\n",
    "#plt.style.use('ggplot') #looks better -compare R Library\n",
    "#bycountry.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Segmenting Customers (Redundant - Model is run in Tableau (more convenient, better visulaizations, quick))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have three Options/Libraries to run a ML-Model:\n",
    "1. ML is generally built around dataframes - we use Spark 2.0 and process data sql.df (preferred)\n",
    "2. MLLIB is built around RDDs\n",
    "3. Sckit-Learn with Pandas\n",
    "\n",
    "Using spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supportingspark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming. Developers should contribute new algorithms to spark.ml if they fit the ML pipeline concept well, e.g., feature extractors and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.clustering import KMeans\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#df = forK #only numeric input for kMeans\n",
    "#cols = df.columns\n",
    "#vectorAss = VectorAssembler(inputCols=cols, outputCol=\"features\") #inputCols=[\"CHILDREN\",\"ADULTS\"]\n",
    "#vdf = vectorAss.transform(df)\n",
    "#vdf = vdf.select('features')\n",
    "#vdf = vdf.limit(10)\n",
    "\n",
    "#kmeans = KMeans(k=2, maxIter=5, seed=1)\n",
    "#kmm = kmeans.fit(vdf)\n",
    "#centers = kmm.clusterCenters()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
